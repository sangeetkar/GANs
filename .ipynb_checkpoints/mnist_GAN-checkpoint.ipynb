{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying **Generative Adversial Nets (GANs)** to MNIST \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "I find **Generative Adversarial Nets framework (GANs)**, [proposed](https://arxiv.org/pdf/1406.2661.pdf) by Ian Goodfellow and his colleagues at University of Montreal, very fascinating. This novel technique relies on **pitching two different deep neural nets against each other** to achieve state-of-the-art results in estimating generative models:\n",
    "\n",
    " - **Generator (G)** which takes a random input and tries to generate an output (fake data) close to the real data (and consequently fool the other DNN)\n",
    " - **Discriminator (D)** which takes as input real data and generated data (fake data) and tries to distinguish one from the other\n",
    "\n",
    "*Both these DNNs try to outdo each other and thus keep improving themselves in their function.*\n",
    "\n",
    "Since a picture is a lot better than a thousand words, here's a diagram that explains it really well:\n",
    "\n",
    "![GAN diagram](./images/GAN.png)\n",
    "\n",
    "## Current Task\n",
    "\n",
    "Here I'll apply this technique to the MNIST dataset and will use the generator to generate some handwritten digits myself. \n",
    "\n",
    "## Approach\n",
    "\n",
    "### Architecture\n",
    "\n",
    "We'll use DNNs mostly comprising of **Fully Connected** layers. We can later improve the architecture perhaps by using Convolutional (CNN) layers.\n",
    "\n",
    "### Deep Learning Library\n",
    "\n",
    "I'll use the [Pytorch Library](http://pytorch.org/) in this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We start with importing the necessary libraries/ packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if GPU is available. We'll use this flag to use GPU for training if available\n",
    "has_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the dataset, dataloader and the transforms that we wish to apply to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data/', train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "We use 3 fully connected layers to model the Disciminator for the time being. Depending on the results we may make it more or less powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = F.relu(self.fc1(inp))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = F.sigmoid(self.fc3(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Here again we use 3 fully connected layers to model the Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 784)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = F.leaky_relu(self.fc1(inp))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        out = F.tanh(self.fc3(x))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "if has_gpu:\n",
    "    discriminator.cuda()\n",
    "    generator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready for the training phase\n",
    "\n",
    "Before the training phase, we define the following:\n",
    "\n",
    "1. Criterion to be used by Loss function : Binary Cross Entropy (because the target label will be binary)\n",
    "2. Adam, my favourite optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss & optimizer\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0005)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the games begin!\n",
    "\n",
    "So now we start the training phase.\n",
    "\n",
    "In each iteration,\n",
    "\n",
    "1. ** Training the Discriminator ** : \n",
    "    - We feed a batch from real data and calculate **Real loss** function (since it's real data the target label should be 1)\n",
    "    - Now we generate a fake image from the generator and feed it to the discriminator calculating **Fake Loss** (with target Label set to 0 since it's fake)\n",
    "    - Now we calculate ** Total Loss ** and use Adam to train the Discriminator w.r.t. this loss\n",
    "    \n",
    "2. ** Training the Generator ** :\n",
    "    - We generate a fake image and and feed it to the Discriminator. \n",
    "    - We calculate the ** Generator loss ** function with the target label set to 1 (since Generator's aim is to generate real image after all)\n",
    "    - We use Adam optimizer to train the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Step [300 / 600], d_loss: 0.2022, g_loss: 3.9815, D(x): 0.98, D(G(z)): 0.15\n",
      "Epoch [0/5], Step [600 / 600], d_loss: 1.0835, g_loss: 2.4868, D(x): 0.69, D(G(z)): 0.37\n",
      "Epoch [1/5], Step [300 / 600], d_loss: 0.3280, g_loss: 3.4601, D(x): 0.86, D(G(z)): 0.10\n",
      "Epoch [1/5], Step [600 / 600], d_loss: 0.8882, g_loss: 2.0701, D(x): 0.70, D(G(z)): 0.36\n",
      "Epoch [2/5], Step [300 / 600], d_loss: 2.9380, g_loss: 1.1472, D(x): 0.36, D(G(z)): 0.62\n",
      "Epoch [2/5], Step [600 / 600], d_loss: 0.5566, g_loss: 1.8488, D(x): 0.79, D(G(z)): 0.25\n",
      "Epoch [3/5], Step [300 / 600], d_loss: 1.9433, g_loss: 0.6596, D(x): 0.54, D(G(z)): 0.65\n",
      "Epoch [3/5], Step [600 / 600], d_loss: 0.5841, g_loss: 2.3619, D(x): 0.71, D(G(z)): 0.16\n",
      "Epoch [4/5], Step [300 / 600], d_loss: 1.4625, g_loss: 1.8418, D(x): 0.62, D(G(z)): 0.42\n",
      "Epoch [4/5], Step [600 / 600], d_loss: 1.0637, g_loss: 1.3700, D(x): 0.64, D(G(z)): 0.40\n",
      "Epoch [5/5], Step [300 / 600], d_loss: 1.5017, g_loss: 1.1628, D(x): 0.53, D(G(z)): 0.43\n",
      "Epoch [5/5], Step [600 / 600], d_loss: 1.2652, g_loss: 1.3246, D(x): 0.66, D(G(z)): 0.48\n",
      "Epoch [6/5], Step [300 / 600], d_loss: 0.5285, g_loss: 2.0429, D(x): 0.77, D(G(z)): 0.21\n",
      "Epoch [6/5], Step [600 / 600], d_loss: 0.6737, g_loss: 1.2163, D(x): 0.75, D(G(z)): 0.31\n",
      "Epoch [7/5], Step [300 / 600], d_loss: 0.8696, g_loss: 1.0793, D(x): 0.78, D(G(z)): 0.43\n",
      "Epoch [7/5], Step [600 / 600], d_loss: 0.6890, g_loss: 1.6215, D(x): 0.79, D(G(z)): 0.30\n",
      "Epoch [8/5], Step [300 / 600], d_loss: 1.3264, g_loss: 2.1058, D(x): 0.72, D(G(z)): 0.41\n",
      "Epoch [8/5], Step [600 / 600], d_loss: 1.3057, g_loss: 4.2376, D(x): 0.81, D(G(z)): 0.39\n",
      "Epoch [9/5], Step [300 / 600], d_loss: 0.2988, g_loss: 2.7919, D(x): 0.91, D(G(z)): 0.16\n",
      "Epoch [9/5], Step [600 / 600], d_loss: 0.3127, g_loss: 2.7164, D(x): 0.92, D(G(z)): 0.16\n",
      "Epoch [10/5], Step [300 / 600], d_loss: 1.7800, g_loss: 0.6712, D(x): 0.55, D(G(z)): 0.55\n",
      "Epoch [10/5], Step [600 / 600], d_loss: 2.0368, g_loss: 1.4652, D(x): 0.47, D(G(z)): 0.35\n",
      "Epoch [11/5], Step [300 / 600], d_loss: 0.7148, g_loss: 2.2307, D(x): 0.79, D(G(z)): 0.31\n",
      "Epoch [11/5], Step [600 / 600], d_loss: 0.5165, g_loss: 2.6024, D(x): 0.78, D(G(z)): 0.17\n",
      "Epoch [12/5], Step [300 / 600], d_loss: 0.8768, g_loss: 3.4032, D(x): 0.79, D(G(z)): 0.33\n",
      "Epoch [12/5], Step [600 / 600], d_loss: 0.6374, g_loss: 5.6942, D(x): 0.73, D(G(z)): 0.10\n",
      "Epoch [13/5], Step [300 / 600], d_loss: 0.5546, g_loss: 2.3545, D(x): 0.81, D(G(z)): 0.22\n",
      "Epoch [13/5], Step [600 / 600], d_loss: 1.3147, g_loss: 1.6363, D(x): 0.75, D(G(z)): 0.46\n",
      "Epoch [14/5], Step [300 / 600], d_loss: 0.6856, g_loss: 2.0686, D(x): 0.73, D(G(z)): 0.23\n",
      "Epoch [14/5], Step [600 / 600], d_loss: 1.0662, g_loss: 2.1252, D(x): 0.73, D(G(z)): 0.30\n",
      "Epoch [15/5], Step [300 / 600], d_loss: 0.5692, g_loss: 4.4282, D(x): 0.76, D(G(z)): 0.14\n",
      "Epoch [15/5], Step [600 / 600], d_loss: 3.3878, g_loss: 1.3889, D(x): 0.31, D(G(z)): 0.52\n",
      "Epoch [16/5], Step [300 / 600], d_loss: 1.0658, g_loss: 1.5193, D(x): 0.68, D(G(z)): 0.39\n",
      "Epoch [16/5], Step [600 / 600], d_loss: 0.8761, g_loss: 2.5429, D(x): 0.76, D(G(z)): 0.26\n",
      "Epoch [17/5], Step [300 / 600], d_loss: 0.6282, g_loss: 3.3094, D(x): 0.83, D(G(z)): 0.20\n",
      "Epoch [17/5], Step [600 / 600], d_loss: 0.6117, g_loss: 1.9874, D(x): 0.76, D(G(z)): 0.18\n",
      "Epoch [18/5], Step [300 / 600], d_loss: 1.0181, g_loss: 1.4718, D(x): 0.68, D(G(z)): 0.34\n",
      "Epoch [18/5], Step [600 / 600], d_loss: 1.0631, g_loss: 1.4348, D(x): 0.70, D(G(z)): 0.41\n",
      "Epoch [19/5], Step [300 / 600], d_loss: 1.2518, g_loss: 1.3447, D(x): 0.70, D(G(z)): 0.47\n",
      "Epoch [19/5], Step [600 / 600], d_loss: 0.9602, g_loss: 1.8114, D(x): 0.69, D(G(z)): 0.33\n",
      "Epoch [20/5], Step [300 / 600], d_loss: 0.6993, g_loss: 2.9651, D(x): 0.73, D(G(z)): 0.16\n",
      "Epoch [20/5], Step [600 / 600], d_loss: 1.4436, g_loss: 1.6950, D(x): 0.60, D(G(z)): 0.33\n",
      "Epoch [21/5], Step [300 / 600], d_loss: 1.1082, g_loss: 1.5665, D(x): 0.70, D(G(z)): 0.40\n",
      "Epoch [21/5], Step [600 / 600], d_loss: 1.1189, g_loss: 1.7073, D(x): 0.59, D(G(z)): 0.21\n",
      "Epoch [22/5], Step [300 / 600], d_loss: 0.6013, g_loss: 2.1334, D(x): 0.82, D(G(z)): 0.26\n",
      "Epoch [22/5], Step [600 / 600], d_loss: 0.5515, g_loss: 2.5950, D(x): 0.84, D(G(z)): 0.22\n",
      "Epoch [23/5], Step [300 / 600], d_loss: 0.4494, g_loss: 3.4497, D(x): 0.87, D(G(z)): 0.15\n",
      "Epoch [23/5], Step [600 / 600], d_loss: 0.8741, g_loss: 2.3241, D(x): 0.73, D(G(z)): 0.30\n",
      "Epoch [24/5], Step [300 / 600], d_loss: 0.7671, g_loss: 2.4539, D(x): 0.73, D(G(z)): 0.20\n",
      "Epoch [24/5], Step [600 / 600], d_loss: 0.7093, g_loss: 3.5360, D(x): 0.74, D(G(z)): 0.13\n",
      "Epoch [25/5], Step [300 / 600], d_loss: 0.7007, g_loss: 3.4726, D(x): 0.80, D(G(z)): 0.17\n",
      "Epoch [25/5], Step [600 / 600], d_loss: 1.0101, g_loss: 1.9320, D(x): 0.80, D(G(z)): 0.38\n",
      "Epoch [26/5], Step [300 / 600], d_loss: 0.6951, g_loss: 1.8787, D(x): 0.80, D(G(z)): 0.24\n",
      "Epoch [26/5], Step [600 / 600], d_loss: 0.4664, g_loss: 2.8699, D(x): 0.84, D(G(z)): 0.14\n",
      "Epoch [27/5], Step [300 / 600], d_loss: 0.7091, g_loss: 2.4637, D(x): 0.77, D(G(z)): 0.23\n",
      "Epoch [27/5], Step [600 / 600], d_loss: 0.7990, g_loss: 3.4556, D(x): 0.73, D(G(z)): 0.14\n",
      "Epoch [28/5], Step [300 / 600], d_loss: 0.9202, g_loss: 1.7213, D(x): 0.78, D(G(z)): 0.28\n",
      "Epoch [28/5], Step [600 / 600], d_loss: 0.6341, g_loss: 2.7755, D(x): 0.81, D(G(z)): 0.20\n",
      "Epoch [29/5], Step [300 / 600], d_loss: 0.7203, g_loss: 2.8582, D(x): 0.72, D(G(z)): 0.14\n",
      "Epoch [29/5], Step [600 / 600], d_loss: 0.7932, g_loss: 1.8251, D(x): 0.82, D(G(z)): 0.29\n",
      "Epoch [30/5], Step [300 / 600], d_loss: 0.9081, g_loss: 1.8651, D(x): 0.73, D(G(z)): 0.24\n",
      "Epoch [30/5], Step [600 / 600], d_loss: 0.6583, g_loss: 2.4472, D(x): 0.79, D(G(z)): 0.19\n",
      "Epoch [31/5], Step [300 / 600], d_loss: 0.6367, g_loss: 2.3069, D(x): 0.79, D(G(z)): 0.16\n",
      "Epoch [31/5], Step [600 / 600], d_loss: 0.6376, g_loss: 4.4129, D(x): 0.75, D(G(z)): 0.14\n",
      "Epoch [32/5], Step [300 / 600], d_loss: 0.6834, g_loss: 2.0212, D(x): 0.77, D(G(z)): 0.19\n",
      "Epoch [32/5], Step [600 / 600], d_loss: 0.8372, g_loss: 2.0669, D(x): 0.74, D(G(z)): 0.21\n",
      "Epoch [33/5], Step [300 / 600], d_loss: 0.6650, g_loss: 2.1569, D(x): 0.79, D(G(z)): 0.18\n",
      "Epoch [33/5], Step [600 / 600], d_loss: 0.7129, g_loss: 2.2015, D(x): 0.83, D(G(z)): 0.28\n",
      "Epoch [34/5], Step [300 / 600], d_loss: 0.6430, g_loss: 2.1148, D(x): 0.82, D(G(z)): 0.23\n",
      "Epoch [34/5], Step [600 / 600], d_loss: 0.4595, g_loss: 2.4701, D(x): 0.88, D(G(z)): 0.20\n",
      "Epoch [35/5], Step [300 / 600], d_loss: 0.7196, g_loss: 2.2170, D(x): 0.89, D(G(z)): 0.32\n",
      "Epoch [35/5], Step [600 / 600], d_loss: 1.0414, g_loss: 1.6635, D(x): 0.71, D(G(z)): 0.26\n",
      "Epoch [36/5], Step [300 / 600], d_loss: 0.6928, g_loss: 1.7044, D(x): 0.82, D(G(z)): 0.27\n",
      "Epoch [36/5], Step [600 / 600], d_loss: 0.5956, g_loss: 2.5242, D(x): 0.82, D(G(z)): 0.21\n",
      "Epoch [37/5], Step [300 / 600], d_loss: 0.6573, g_loss: 1.5032, D(x): 0.80, D(G(z)): 0.21\n",
      "Epoch [37/5], Step [600 / 600], d_loss: 0.5044, g_loss: 2.7310, D(x): 0.83, D(G(z)): 0.17\n",
      "Epoch [38/5], Step [300 / 600], d_loss: 0.4273, g_loss: 2.5686, D(x): 0.89, D(G(z)): 0.17\n",
      "Epoch [38/5], Step [600 / 600], d_loss: 0.4981, g_loss: 2.1518, D(x): 0.85, D(G(z)): 0.19\n",
      "Epoch [39/5], Step [300 / 600], d_loss: 0.6534, g_loss: 2.9404, D(x): 0.77, D(G(z)): 0.17\n",
      "Epoch [39/5], Step [600 / 600], d_loss: 0.9357, g_loss: 2.8924, D(x): 0.72, D(G(z)): 0.21\n",
      "Epoch [40/5], Step [300 / 600], d_loss: 0.5560, g_loss: 2.4111, D(x): 0.82, D(G(z)): 0.22\n",
      "Epoch [40/5], Step [600 / 600], d_loss: 0.7325, g_loss: 1.8623, D(x): 0.78, D(G(z)): 0.27\n",
      "Epoch [41/5], Step [300 / 600], d_loss: 0.7143, g_loss: 2.0160, D(x): 0.80, D(G(z)): 0.26\n",
      "Epoch [41/5], Step [600 / 600], d_loss: 0.6232, g_loss: 2.3487, D(x): 0.85, D(G(z)): 0.26\n",
      "Epoch [42/5], Step [300 / 600], d_loss: 0.8358, g_loss: 2.0769, D(x): 0.81, D(G(z)): 0.33\n",
      "Epoch [42/5], Step [600 / 600], d_loss: 0.6957, g_loss: 2.0461, D(x): 0.80, D(G(z)): 0.23\n",
      "Epoch [43/5], Step [300 / 600], d_loss: 0.7682, g_loss: 2.0369, D(x): 0.74, D(G(z)): 0.20\n",
      "Epoch [43/5], Step [600 / 600], d_loss: 0.8787, g_loss: 1.5440, D(x): 0.71, D(G(z)): 0.26\n",
      "Epoch [44/5], Step [300 / 600], d_loss: 0.7486, g_loss: 2.0040, D(x): 0.74, D(G(z)): 0.23\n",
      "Epoch [44/5], Step [600 / 600], d_loss: 0.6083, g_loss: 2.3847, D(x): 0.81, D(G(z)): 0.23\n",
      "Epoch [45/5], Step [300 / 600], d_loss: 0.8499, g_loss: 1.8260, D(x): 0.77, D(G(z)): 0.31\n",
      "Epoch [45/5], Step [600 / 600], d_loss: 0.6799, g_loss: 1.6581, D(x): 0.77, D(G(z)): 0.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/5], Step [300 / 600], d_loss: 0.9178, g_loss: 1.8897, D(x): 0.73, D(G(z)): 0.30\n",
      "Epoch [46/5], Step [600 / 600], d_loss: 0.7758, g_loss: 1.5473, D(x): 0.82, D(G(z)): 0.34\n",
      "Epoch [47/5], Step [300 / 600], d_loss: 0.8377, g_loss: 1.7893, D(x): 0.74, D(G(z)): 0.31\n",
      "Epoch [47/5], Step [600 / 600], d_loss: 0.8023, g_loss: 1.7132, D(x): 0.71, D(G(z)): 0.24\n",
      "Epoch [48/5], Step [300 / 600], d_loss: 0.8281, g_loss: 1.7613, D(x): 0.75, D(G(z)): 0.29\n",
      "Epoch [48/5], Step [600 / 600], d_loss: 0.8203, g_loss: 1.5359, D(x): 0.75, D(G(z)): 0.31\n",
      "Epoch [49/5], Step [300 / 600], d_loss: 0.8987, g_loss: 1.6966, D(x): 0.76, D(G(z)): 0.32\n",
      "Epoch [49/5], Step [600 / 600], d_loss: 0.8090, g_loss: 1.8267, D(x): 0.73, D(G(z)): 0.28\n",
      "Epoch [50/5], Step [300 / 600], d_loss: 1.0110, g_loss: 1.4934, D(x): 0.70, D(G(z)): 0.37\n",
      "Epoch [50/5], Step [600 / 600], d_loss: 1.0230, g_loss: 1.6370, D(x): 0.64, D(G(z)): 0.28\n",
      "Epoch [51/5], Step [300 / 600], d_loss: 1.0921, g_loss: 1.7343, D(x): 0.65, D(G(z)): 0.29\n",
      "Epoch [51/5], Step [600 / 600], d_loss: 1.3181, g_loss: 1.0622, D(x): 0.73, D(G(z)): 0.48\n",
      "Epoch [52/5], Step [300 / 600], d_loss: 0.8038, g_loss: 2.2072, D(x): 0.76, D(G(z)): 0.31\n",
      "Epoch [52/5], Step [600 / 600], d_loss: 1.3110, g_loss: 1.3880, D(x): 0.67, D(G(z)): 0.40\n",
      "Epoch [53/5], Step [300 / 600], d_loss: 0.9197, g_loss: 1.3134, D(x): 0.74, D(G(z)): 0.36\n",
      "Epoch [53/5], Step [600 / 600], d_loss: 0.9847, g_loss: 1.5348, D(x): 0.65, D(G(z)): 0.31\n",
      "Epoch [54/5], Step [300 / 600], d_loss: 0.9393, g_loss: 1.7699, D(x): 0.62, D(G(z)): 0.25\n",
      "Epoch [54/5], Step [600 / 600], d_loss: 1.0289, g_loss: 1.1971, D(x): 0.74, D(G(z)): 0.41\n",
      "Epoch [55/5], Step [300 / 600], d_loss: 0.9076, g_loss: 1.5507, D(x): 0.71, D(G(z)): 0.32\n",
      "Epoch [55/5], Step [600 / 600], d_loss: 1.0481, g_loss: 1.5165, D(x): 0.67, D(G(z)): 0.33\n",
      "Epoch [56/5], Step [300 / 600], d_loss: 0.9097, g_loss: 1.2901, D(x): 0.69, D(G(z)): 0.33\n",
      "Epoch [56/5], Step [600 / 600], d_loss: 0.8734, g_loss: 1.8423, D(x): 0.67, D(G(z)): 0.25\n",
      "Epoch [57/5], Step [300 / 600], d_loss: 0.8813, g_loss: 1.4695, D(x): 0.69, D(G(z)): 0.31\n",
      "Epoch [57/5], Step [600 / 600], d_loss: 1.1066, g_loss: 1.2533, D(x): 0.68, D(G(z)): 0.37\n",
      "Epoch [58/5], Step [300 / 600], d_loss: 0.9664, g_loss: 1.6949, D(x): 0.69, D(G(z)): 0.32\n",
      "Epoch [58/5], Step [600 / 600], d_loss: 0.9875, g_loss: 1.2954, D(x): 0.67, D(G(z)): 0.33\n",
      "Epoch [59/5], Step [300 / 600], d_loss: 1.0601, g_loss: 1.1732, D(x): 0.71, D(G(z)): 0.39\n",
      "Epoch [59/5], Step [600 / 600], d_loss: 0.9111, g_loss: 1.4771, D(x): 0.70, D(G(z)): 0.32\n",
      "Epoch [60/5], Step [300 / 600], d_loss: 1.0475, g_loss: 1.5109, D(x): 0.68, D(G(z)): 0.39\n",
      "Epoch [60/5], Step [600 / 600], d_loss: 0.8271, g_loss: 1.7605, D(x): 0.69, D(G(z)): 0.26\n",
      "Epoch [61/5], Step [300 / 600], d_loss: 0.9655, g_loss: 1.5404, D(x): 0.66, D(G(z)): 0.30\n",
      "Epoch [61/5], Step [600 / 600], d_loss: 1.0094, g_loss: 1.5719, D(x): 0.64, D(G(z)): 0.27\n",
      "Epoch [62/5], Step [300 / 600], d_loss: 0.8995, g_loss: 1.6367, D(x): 0.65, D(G(z)): 0.26\n",
      "Epoch [62/5], Step [600 / 600], d_loss: 1.0414, g_loss: 1.4453, D(x): 0.61, D(G(z)): 0.30\n",
      "Epoch [63/5], Step [300 / 600], d_loss: 1.0958, g_loss: 1.5618, D(x): 0.61, D(G(z)): 0.28\n",
      "Epoch [63/5], Step [600 / 600], d_loss: 0.9733, g_loss: 1.5542, D(x): 0.63, D(G(z)): 0.26\n",
      "Epoch [64/5], Step [300 / 600], d_loss: 0.8886, g_loss: 1.5450, D(x): 0.67, D(G(z)): 0.27\n",
      "Epoch [64/5], Step [600 / 600], d_loss: 1.1418, g_loss: 1.1526, D(x): 0.62, D(G(z)): 0.37\n",
      "Epoch [65/5], Step [300 / 600], d_loss: 1.0245, g_loss: 1.2956, D(x): 0.67, D(G(z)): 0.34\n",
      "Epoch [65/5], Step [600 / 600], d_loss: 0.9395, g_loss: 1.3052, D(x): 0.65, D(G(z)): 0.31\n",
      "Epoch [66/5], Step [300 / 600], d_loss: 0.9905, g_loss: 1.5249, D(x): 0.64, D(G(z)): 0.27\n",
      "Epoch [66/5], Step [600 / 600], d_loss: 1.0922, g_loss: 1.5833, D(x): 0.67, D(G(z)): 0.38\n",
      "Epoch [67/5], Step [300 / 600], d_loss: 0.8973, g_loss: 1.5161, D(x): 0.64, D(G(z)): 0.28\n",
      "Epoch [67/5], Step [600 / 600], d_loss: 0.9948, g_loss: 1.5645, D(x): 0.68, D(G(z)): 0.35\n",
      "Epoch [68/5], Step [300 / 600], d_loss: 0.9779, g_loss: 1.6482, D(x): 0.67, D(G(z)): 0.32\n",
      "Epoch [68/5], Step [600 / 600], d_loss: 1.2865, g_loss: 1.1418, D(x): 0.69, D(G(z)): 0.50\n",
      "Epoch [69/5], Step [300 / 600], d_loss: 0.9931, g_loss: 1.1529, D(x): 0.67, D(G(z)): 0.34\n",
      "Epoch [69/5], Step [600 / 600], d_loss: 1.0823, g_loss: 1.3479, D(x): 0.65, D(G(z)): 0.38\n",
      "Epoch [70/5], Step [300 / 600], d_loss: 0.9305, g_loss: 1.4242, D(x): 0.68, D(G(z)): 0.32\n",
      "Epoch [70/5], Step [600 / 600], d_loss: 1.0361, g_loss: 1.2096, D(x): 0.64, D(G(z)): 0.35\n",
      "Epoch [71/5], Step [300 / 600], d_loss: 0.8865, g_loss: 1.5215, D(x): 0.67, D(G(z)): 0.31\n",
      "Epoch [71/5], Step [600 / 600], d_loss: 0.9696, g_loss: 1.5410, D(x): 0.69, D(G(z)): 0.34\n",
      "Epoch [72/5], Step [300 / 600], d_loss: 1.0818, g_loss: 1.3106, D(x): 0.66, D(G(z)): 0.40\n",
      "Epoch [72/5], Step [600 / 600], d_loss: 1.1330, g_loss: 1.3935, D(x): 0.62, D(G(z)): 0.34\n",
      "Epoch [73/5], Step [300 / 600], d_loss: 1.0006, g_loss: 1.2896, D(x): 0.64, D(G(z)): 0.34\n",
      "Epoch [73/5], Step [600 / 600], d_loss: 1.1861, g_loss: 1.3722, D(x): 0.58, D(G(z)): 0.34\n",
      "Epoch [74/5], Step [300 / 600], d_loss: 1.0546, g_loss: 1.3415, D(x): 0.62, D(G(z)): 0.33\n",
      "Epoch [74/5], Step [600 / 600], d_loss: 1.2242, g_loss: 1.0428, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [75/5], Step [300 / 600], d_loss: 1.1026, g_loss: 1.2369, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [75/5], Step [600 / 600], d_loss: 1.1184, g_loss: 1.3023, D(x): 0.66, D(G(z)): 0.40\n",
      "Epoch [76/5], Step [300 / 600], d_loss: 1.2701, g_loss: 0.9869, D(x): 0.59, D(G(z)): 0.39\n",
      "Epoch [76/5], Step [600 / 600], d_loss: 1.1711, g_loss: 1.1560, D(x): 0.66, D(G(z)): 0.42\n",
      "Epoch [77/5], Step [300 / 600], d_loss: 1.0971, g_loss: 1.0222, D(x): 0.68, D(G(z)): 0.42\n",
      "Epoch [77/5], Step [600 / 600], d_loss: 1.3589, g_loss: 1.1279, D(x): 0.56, D(G(z)): 0.40\n",
      "Epoch [78/5], Step [300 / 600], d_loss: 1.1506, g_loss: 1.1034, D(x): 0.66, D(G(z)): 0.41\n",
      "Epoch [78/5], Step [600 / 600], d_loss: 1.1532, g_loss: 1.4451, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [79/5], Step [300 / 600], d_loss: 1.1946, g_loss: 1.1223, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [79/5], Step [600 / 600], d_loss: 1.0877, g_loss: 1.3042, D(x): 0.61, D(G(z)): 0.36\n",
      "Epoch [80/5], Step [300 / 600], d_loss: 1.1195, g_loss: 1.1588, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [80/5], Step [600 / 600], d_loss: 1.0684, g_loss: 1.0110, D(x): 0.68, D(G(z)): 0.41\n",
      "Epoch [81/5], Step [300 / 600], d_loss: 1.2645, g_loss: 1.1814, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [81/5], Step [600 / 600], d_loss: 1.0007, g_loss: 1.3602, D(x): 0.63, D(G(z)): 0.33\n",
      "Epoch [82/5], Step [300 / 600], d_loss: 1.0366, g_loss: 1.1067, D(x): 0.65, D(G(z)): 0.37\n",
      "Epoch [82/5], Step [600 / 600], d_loss: 1.1038, g_loss: 1.5196, D(x): 0.58, D(G(z)): 0.30\n",
      "Epoch [83/5], Step [300 / 600], d_loss: 1.0352, g_loss: 1.2420, D(x): 0.65, D(G(z)): 0.36\n",
      "Epoch [83/5], Step [600 / 600], d_loss: 0.9951, g_loss: 1.3607, D(x): 0.66, D(G(z)): 0.37\n",
      "Epoch [84/5], Step [300 / 600], d_loss: 1.1762, g_loss: 1.3150, D(x): 0.62, D(G(z)): 0.39\n",
      "Epoch [84/5], Step [600 / 600], d_loss: 1.0098, g_loss: 1.3236, D(x): 0.70, D(G(z)): 0.41\n",
      "Epoch [85/5], Step [300 / 600], d_loss: 1.0404, g_loss: 1.2609, D(x): 0.62, D(G(z)): 0.34\n",
      "Epoch [85/5], Step [600 / 600], d_loss: 1.1166, g_loss: 1.0993, D(x): 0.58, D(G(z)): 0.32\n",
      "Epoch [86/5], Step [300 / 600], d_loss: 1.1254, g_loss: 1.2315, D(x): 0.65, D(G(z)): 0.42\n",
      "Epoch [86/5], Step [600 / 600], d_loss: 1.2024, g_loss: 1.1935, D(x): 0.59, D(G(z)): 0.41\n",
      "Epoch [87/5], Step [300 / 600], d_loss: 1.0210, g_loss: 1.4007, D(x): 0.65, D(G(z)): 0.36\n",
      "Epoch [87/5], Step [600 / 600], d_loss: 0.9145, g_loss: 1.2513, D(x): 0.69, D(G(z)): 0.34\n",
      "Epoch [88/5], Step [300 / 600], d_loss: 1.2708, g_loss: 1.3658, D(x): 0.56, D(G(z)): 0.37\n",
      "Epoch [88/5], Step [600 / 600], d_loss: 1.1490, g_loss: 1.1603, D(x): 0.63, D(G(z)): 0.40\n",
      "Epoch [89/5], Step [300 / 600], d_loss: 1.1125, g_loss: 1.0948, D(x): 0.64, D(G(z)): 0.40\n",
      "Epoch [89/5], Step [600 / 600], d_loss: 1.1672, g_loss: 1.2276, D(x): 0.59, D(G(z)): 0.38\n",
      "Epoch [90/5], Step [300 / 600], d_loss: 1.3082, g_loss: 1.0397, D(x): 0.60, D(G(z)): 0.46\n",
      "Epoch [90/5], Step [600 / 600], d_loss: 1.0791, g_loss: 1.3243, D(x): 0.59, D(G(z)): 0.34\n",
      "Epoch [91/5], Step [300 / 600], d_loss: 1.1954, g_loss: 1.2229, D(x): 0.61, D(G(z)): 0.41\n",
      "Epoch [91/5], Step [600 / 600], d_loss: 1.1569, g_loss: 1.1950, D(x): 0.58, D(G(z)): 0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/5], Step [300 / 600], d_loss: 1.1118, g_loss: 1.1675, D(x): 0.64, D(G(z)): 0.39\n",
      "Epoch [92/5], Step [600 / 600], d_loss: 1.1132, g_loss: 1.2288, D(x): 0.64, D(G(z)): 0.41\n",
      "Epoch [93/5], Step [300 / 600], d_loss: 1.1609, g_loss: 1.2391, D(x): 0.57, D(G(z)): 0.34\n",
      "Epoch [93/5], Step [600 / 600], d_loss: 1.1668, g_loss: 1.2965, D(x): 0.61, D(G(z)): 0.39\n",
      "Epoch [94/5], Step [300 / 600], d_loss: 1.0974, g_loss: 1.2885, D(x): 0.58, D(G(z)): 0.33\n",
      "Epoch [94/5], Step [600 / 600], d_loss: 1.1334, g_loss: 1.4101, D(x): 0.60, D(G(z)): 0.39\n",
      "Epoch [95/5], Step [300 / 600], d_loss: 1.2594, g_loss: 1.3788, D(x): 0.53, D(G(z)): 0.32\n",
      "Epoch [95/5], Step [600 / 600], d_loss: 1.0774, g_loss: 1.0359, D(x): 0.63, D(G(z)): 0.38\n",
      "Epoch [96/5], Step [300 / 600], d_loss: 0.9847, g_loss: 1.1312, D(x): 0.69, D(G(z)): 0.38\n",
      "Epoch [96/5], Step [600 / 600], d_loss: 1.1621, g_loss: 1.3665, D(x): 0.63, D(G(z)): 0.42\n",
      "Epoch [97/5], Step [300 / 600], d_loss: 1.1383, g_loss: 1.2375, D(x): 0.58, D(G(z)): 0.38\n",
      "Epoch [97/5], Step [600 / 600], d_loss: 1.2348, g_loss: 0.7813, D(x): 0.67, D(G(z)): 0.50\n",
      "Epoch [98/5], Step [300 / 600], d_loss: 1.0666, g_loss: 1.2236, D(x): 0.70, D(G(z)): 0.44\n",
      "Epoch [98/5], Step [600 / 600], d_loss: 1.2002, g_loss: 1.1958, D(x): 0.57, D(G(z)): 0.37\n",
      "Epoch [99/5], Step [300 / 600], d_loss: 1.2590, g_loss: 1.1472, D(x): 0.57, D(G(z)): 0.40\n",
      "Epoch [99/5], Step [600 / 600], d_loss: 1.0935, g_loss: 0.9062, D(x): 0.65, D(G(z)): 0.42\n",
      "Epoch [100/5], Step [300 / 600], d_loss: 1.2460, g_loss: 1.2266, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [100/5], Step [600 / 600], d_loss: 1.1198, g_loss: 1.3565, D(x): 0.60, D(G(z)): 0.38\n",
      "Epoch [101/5], Step [300 / 600], d_loss: 1.1036, g_loss: 1.2447, D(x): 0.59, D(G(z)): 0.35\n",
      "Epoch [101/5], Step [600 / 600], d_loss: 1.0102, g_loss: 1.3836, D(x): 0.65, D(G(z)): 0.32\n",
      "Epoch [102/5], Step [300 / 600], d_loss: 1.1399, g_loss: 1.0749, D(x): 0.62, D(G(z)): 0.41\n",
      "Epoch [102/5], Step [600 / 600], d_loss: 1.1744, g_loss: 1.1824, D(x): 0.68, D(G(z)): 0.47\n",
      "Epoch [103/5], Step [300 / 600], d_loss: 1.1458, g_loss: 1.2934, D(x): 0.59, D(G(z)): 0.38\n",
      "Epoch [103/5], Step [600 / 600], d_loss: 1.1719, g_loss: 1.0905, D(x): 0.61, D(G(z)): 0.40\n",
      "Epoch [104/5], Step [300 / 600], d_loss: 1.1743, g_loss: 1.0794, D(x): 0.58, D(G(z)): 0.38\n",
      "Epoch [104/5], Step [600 / 600], d_loss: 1.1994, g_loss: 1.1048, D(x): 0.63, D(G(z)): 0.43\n",
      "Epoch [105/5], Step [300 / 600], d_loss: 1.0517, g_loss: 1.2088, D(x): 0.63, D(G(z)): 0.35\n",
      "Epoch [105/5], Step [600 / 600], d_loss: 1.1671, g_loss: 1.1594, D(x): 0.61, D(G(z)): 0.39\n",
      "Epoch [106/5], Step [300 / 600], d_loss: 1.1887, g_loss: 1.0842, D(x): 0.61, D(G(z)): 0.41\n",
      "Epoch [106/5], Step [600 / 600], d_loss: 1.2049, g_loss: 1.1904, D(x): 0.56, D(G(z)): 0.36\n",
      "Epoch [107/5], Step [300 / 600], d_loss: 1.2879, g_loss: 1.1320, D(x): 0.58, D(G(z)): 0.43\n",
      "Epoch [107/5], Step [600 / 600], d_loss: 1.1660, g_loss: 1.2054, D(x): 0.63, D(G(z)): 0.43\n",
      "Epoch [108/5], Step [300 / 600], d_loss: 1.1489, g_loss: 0.9314, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [108/5], Step [600 / 600], d_loss: 0.9839, g_loss: 1.1347, D(x): 0.68, D(G(z)): 0.38\n",
      "Epoch [109/5], Step [300 / 600], d_loss: 1.1859, g_loss: 1.0288, D(x): 0.56, D(G(z)): 0.36\n",
      "Epoch [109/5], Step [600 / 600], d_loss: 1.0373, g_loss: 1.1486, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [110/5], Step [300 / 600], d_loss: 1.1966, g_loss: 1.1347, D(x): 0.56, D(G(z)): 0.36\n",
      "Epoch [110/5], Step [600 / 600], d_loss: 1.1617, g_loss: 1.1065, D(x): 0.61, D(G(z)): 0.39\n",
      "Epoch [111/5], Step [300 / 600], d_loss: 1.1669, g_loss: 1.0168, D(x): 0.62, D(G(z)): 0.40\n",
      "Epoch [111/5], Step [600 / 600], d_loss: 1.1344, g_loss: 1.2264, D(x): 0.65, D(G(z)): 0.40\n",
      "Epoch [112/5], Step [300 / 600], d_loss: 1.3573, g_loss: 0.8886, D(x): 0.56, D(G(z)): 0.43\n",
      "Epoch [112/5], Step [600 / 600], d_loss: 1.1730, g_loss: 1.0584, D(x): 0.62, D(G(z)): 0.43\n",
      "Epoch [113/5], Step [300 / 600], d_loss: 1.2082, g_loss: 1.3028, D(x): 0.51, D(G(z)): 0.32\n",
      "Epoch [113/5], Step [600 / 600], d_loss: 1.1089, g_loss: 1.0577, D(x): 0.64, D(G(z)): 0.39\n",
      "Epoch [114/5], Step [300 / 600], d_loss: 1.2440, g_loss: 1.1907, D(x): 0.57, D(G(z)): 0.38\n",
      "Epoch [114/5], Step [600 / 600], d_loss: 0.9990, g_loss: 1.1439, D(x): 0.69, D(G(z)): 0.41\n",
      "Epoch [115/5], Step [300 / 600], d_loss: 1.1747, g_loss: 1.1055, D(x): 0.62, D(G(z)): 0.40\n",
      "Epoch [115/5], Step [600 / 600], d_loss: 1.2123, g_loss: 1.1790, D(x): 0.58, D(G(z)): 0.38\n",
      "Epoch [116/5], Step [300 / 600], d_loss: 1.3181, g_loss: 1.1811, D(x): 0.56, D(G(z)): 0.41\n",
      "Epoch [116/5], Step [600 / 600], d_loss: 1.0650, g_loss: 1.1027, D(x): 0.66, D(G(z)): 0.38\n",
      "Epoch [117/5], Step [300 / 600], d_loss: 1.0750, g_loss: 1.3422, D(x): 0.64, D(G(z)): 0.39\n",
      "Epoch [117/5], Step [600 / 600], d_loss: 1.0501, g_loss: 0.9696, D(x): 0.70, D(G(z)): 0.43\n",
      "Epoch [118/5], Step [300 / 600], d_loss: 1.2927, g_loss: 0.9928, D(x): 0.59, D(G(z)): 0.43\n",
      "Epoch [118/5], Step [600 / 600], d_loss: 1.1917, g_loss: 1.1559, D(x): 0.56, D(G(z)): 0.36\n",
      "Epoch [119/5], Step [300 / 600], d_loss: 1.0260, g_loss: 1.0360, D(x): 0.66, D(G(z)): 0.38\n",
      "Epoch [119/5], Step [600 / 600], d_loss: 1.1176, g_loss: 1.2422, D(x): 0.62, D(G(z)): 0.39\n",
      "Epoch [120/5], Step [300 / 600], d_loss: 1.1198, g_loss: 1.4969, D(x): 0.63, D(G(z)): 0.39\n",
      "Epoch [120/5], Step [600 / 600], d_loss: 1.1363, g_loss: 0.9298, D(x): 0.63, D(G(z)): 0.39\n",
      "Epoch [121/5], Step [300 / 600], d_loss: 1.1241, g_loss: 1.0485, D(x): 0.62, D(G(z)): 0.38\n",
      "Epoch [121/5], Step [600 / 600], d_loss: 1.2278, g_loss: 0.9933, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [122/5], Step [300 / 600], d_loss: 1.1802, g_loss: 0.9544, D(x): 0.61, D(G(z)): 0.42\n",
      "Epoch [122/5], Step [600 / 600], d_loss: 1.1560, g_loss: 1.2571, D(x): 0.60, D(G(z)): 0.38\n",
      "Epoch [123/5], Step [300 / 600], d_loss: 1.1863, g_loss: 0.9239, D(x): 0.59, D(G(z)): 0.41\n",
      "Epoch [123/5], Step [600 / 600], d_loss: 1.1186, g_loss: 1.1989, D(x): 0.63, D(G(z)): 0.39\n",
      "Epoch [124/5], Step [300 / 600], d_loss: 1.1073, g_loss: 1.0956, D(x): 0.63, D(G(z)): 0.41\n",
      "Epoch [124/5], Step [600 / 600], d_loss: 1.1064, g_loss: 1.3037, D(x): 0.63, D(G(z)): 0.38\n",
      "Epoch [125/5], Step [300 / 600], d_loss: 1.0561, g_loss: 1.0522, D(x): 0.61, D(G(z)): 0.34\n",
      "Epoch [125/5], Step [600 / 600], d_loss: 1.1803, g_loss: 1.0176, D(x): 0.60, D(G(z)): 0.38\n",
      "Epoch [126/5], Step [300 / 600], d_loss: 1.0559, g_loss: 1.2909, D(x): 0.58, D(G(z)): 0.32\n",
      "Epoch [126/5], Step [600 / 600], d_loss: 1.0789, g_loss: 1.2149, D(x): 0.65, D(G(z)): 0.41\n",
      "Epoch [127/5], Step [300 / 600], d_loss: 1.1090, g_loss: 0.9290, D(x): 0.65, D(G(z)): 0.42\n",
      "Epoch [127/5], Step [600 / 600], d_loss: 1.1413, g_loss: 1.1931, D(x): 0.59, D(G(z)): 0.37\n",
      "Epoch [128/5], Step [300 / 600], d_loss: 1.0364, g_loss: 1.0592, D(x): 0.68, D(G(z)): 0.40\n",
      "Epoch [128/5], Step [600 / 600], d_loss: 1.1396, g_loss: 1.0258, D(x): 0.60, D(G(z)): 0.39\n",
      "Epoch [129/5], Step [300 / 600], d_loss: 1.1870, g_loss: 1.2112, D(x): 0.55, D(G(z)): 0.36\n",
      "Epoch [129/5], Step [600 / 600], d_loss: 1.0694, g_loss: 1.2852, D(x): 0.61, D(G(z)): 0.34\n",
      "Epoch [130/5], Step [300 / 600], d_loss: 1.2214, g_loss: 1.0883, D(x): 0.59, D(G(z)): 0.41\n",
      "Epoch [130/5], Step [600 / 600], d_loss: 1.0420, g_loss: 1.1187, D(x): 0.68, D(G(z)): 0.40\n",
      "Epoch [131/5], Step [300 / 600], d_loss: 1.0612, g_loss: 1.4210, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [131/5], Step [600 / 600], d_loss: 1.1071, g_loss: 1.3159, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [132/5], Step [300 / 600], d_loss: 1.0520, g_loss: 0.9484, D(x): 0.61, D(G(z)): 0.35\n",
      "Epoch [132/5], Step [600 / 600], d_loss: 1.3474, g_loss: 1.0601, D(x): 0.53, D(G(z)): 0.40\n",
      "Epoch [133/5], Step [300 / 600], d_loss: 1.0788, g_loss: 1.1379, D(x): 0.62, D(G(z)): 0.36\n",
      "Epoch [133/5], Step [600 / 600], d_loss: 1.2136, g_loss: 0.9809, D(x): 0.62, D(G(z)): 0.44\n",
      "Epoch [134/5], Step [300 / 600], d_loss: 1.1268, g_loss: 1.1778, D(x): 0.59, D(G(z)): 0.38\n",
      "Epoch [134/5], Step [600 / 600], d_loss: 1.0960, g_loss: 1.1659, D(x): 0.61, D(G(z)): 0.36\n",
      "Epoch [135/5], Step [300 / 600], d_loss: 1.2195, g_loss: 1.1540, D(x): 0.59, D(G(z)): 0.42\n",
      "Epoch [135/5], Step [600 / 600], d_loss: 1.1807, g_loss: 1.1547, D(x): 0.55, D(G(z)): 0.35\n",
      "Epoch [136/5], Step [300 / 600], d_loss: 1.1967, g_loss: 1.0992, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [136/5], Step [600 / 600], d_loss: 1.1988, g_loss: 1.1177, D(x): 0.63, D(G(z)): 0.43\n",
      "Epoch [137/5], Step [300 / 600], d_loss: 1.2621, g_loss: 1.0641, D(x): 0.60, D(G(z)): 0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [137/5], Step [600 / 600], d_loss: 1.1095, g_loss: 1.2872, D(x): 0.59, D(G(z)): 0.35\n",
      "Epoch [138/5], Step [300 / 600], d_loss: 1.1475, g_loss: 1.2265, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [138/5], Step [600 / 600], d_loss: 1.2015, g_loss: 1.2632, D(x): 0.58, D(G(z)): 0.38\n",
      "Epoch [139/5], Step [300 / 600], d_loss: 1.0819, g_loss: 1.0834, D(x): 0.59, D(G(z)): 0.34\n",
      "Epoch [139/5], Step [600 / 600], d_loss: 1.0844, g_loss: 1.2339, D(x): 0.64, D(G(z)): 0.38\n",
      "Epoch [140/5], Step [300 / 600], d_loss: 1.0369, g_loss: 1.0390, D(x): 0.67, D(G(z)): 0.39\n",
      "Epoch [140/5], Step [600 / 600], d_loss: 1.0806, g_loss: 1.1119, D(x): 0.60, D(G(z)): 0.36\n",
      "Epoch [141/5], Step [300 / 600], d_loss: 1.1824, g_loss: 1.1233, D(x): 0.60, D(G(z)): 0.41\n",
      "Epoch [141/5], Step [600 / 600], d_loss: 1.0818, g_loss: 1.0752, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [142/5], Step [300 / 600], d_loss: 1.1400, g_loss: 1.1554, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [142/5], Step [600 / 600], d_loss: 1.1607, g_loss: 1.0108, D(x): 0.63, D(G(z)): 0.42\n",
      "Epoch [143/5], Step [300 / 600], d_loss: 1.1383, g_loss: 1.2065, D(x): 0.65, D(G(z)): 0.41\n",
      "Epoch [143/5], Step [600 / 600], d_loss: 1.2254, g_loss: 0.9846, D(x): 0.65, D(G(z)): 0.46\n",
      "Epoch [144/5], Step [300 / 600], d_loss: 1.0734, g_loss: 1.1732, D(x): 0.64, D(G(z)): 0.39\n",
      "Epoch [144/5], Step [600 / 600], d_loss: 1.1891, g_loss: 0.9165, D(x): 0.65, D(G(z)): 0.45\n",
      "Epoch [145/5], Step [300 / 600], d_loss: 1.0707, g_loss: 1.1211, D(x): 0.66, D(G(z)): 0.41\n",
      "Epoch [145/5], Step [600 / 600], d_loss: 1.0931, g_loss: 1.0444, D(x): 0.62, D(G(z)): 0.38\n",
      "Epoch [146/5], Step [300 / 600], d_loss: 1.0683, g_loss: 1.1787, D(x): 0.63, D(G(z)): 0.37\n",
      "Epoch [146/5], Step [600 / 600], d_loss: 1.0542, g_loss: 1.0219, D(x): 0.62, D(G(z)): 0.35\n",
      "Epoch [147/5], Step [300 / 600], d_loss: 1.0732, g_loss: 1.0510, D(x): 0.70, D(G(z)): 0.44\n",
      "Epoch [147/5], Step [600 / 600], d_loss: 1.2413, g_loss: 1.1069, D(x): 0.63, D(G(z)): 0.46\n",
      "Epoch [148/5], Step [300 / 600], d_loss: 1.0960, g_loss: 1.2399, D(x): 0.60, D(G(z)): 0.35\n",
      "Epoch [148/5], Step [600 / 600], d_loss: 1.2353, g_loss: 0.8866, D(x): 0.65, D(G(z)): 0.46\n",
      "Epoch [149/5], Step [300 / 600], d_loss: 1.1069, g_loss: 1.0201, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [149/5], Step [600 / 600], d_loss: 1.1697, g_loss: 1.1352, D(x): 0.55, D(G(z)): 0.33\n",
      "Epoch [150/5], Step [300 / 600], d_loss: 1.0221, g_loss: 1.2007, D(x): 0.63, D(G(z)): 0.35\n",
      "Epoch [150/5], Step [600 / 600], d_loss: 1.0594, g_loss: 0.9877, D(x): 0.63, D(G(z)): 0.38\n",
      "Epoch [151/5], Step [300 / 600], d_loss: 1.2242, g_loss: 1.0453, D(x): 0.58, D(G(z)): 0.41\n",
      "Epoch [151/5], Step [600 / 600], d_loss: 1.2046, g_loss: 1.1784, D(x): 0.63, D(G(z)): 0.42\n",
      "Epoch [152/5], Step [300 / 600], d_loss: 1.0820, g_loss: 1.0488, D(x): 0.59, D(G(z)): 0.35\n",
      "Epoch [152/5], Step [600 / 600], d_loss: 1.1594, g_loss: 1.3930, D(x): 0.58, D(G(z)): 0.37\n",
      "Epoch [153/5], Step [300 / 600], d_loss: 1.2497, g_loss: 1.0031, D(x): 0.59, D(G(z)): 0.43\n",
      "Epoch [153/5], Step [600 / 600], d_loss: 1.2449, g_loss: 0.9693, D(x): 0.56, D(G(z)): 0.37\n",
      "Epoch [154/5], Step [300 / 600], d_loss: 1.1208, g_loss: 1.3626, D(x): 0.58, D(G(z)): 0.35\n",
      "Epoch [154/5], Step [600 / 600], d_loss: 1.1569, g_loss: 0.9698, D(x): 0.61, D(G(z)): 0.39\n",
      "Epoch [155/5], Step [300 / 600], d_loss: 1.2008, g_loss: 1.0197, D(x): 0.66, D(G(z)): 0.48\n",
      "Epoch [155/5], Step [600 / 600], d_loss: 1.2329, g_loss: 1.0589, D(x): 0.60, D(G(z)): 0.44\n",
      "Epoch [156/5], Step [300 / 600], d_loss: 1.2061, g_loss: 0.8970, D(x): 0.61, D(G(z)): 0.43\n",
      "Epoch [156/5], Step [600 / 600], d_loss: 1.1296, g_loss: 1.1251, D(x): 0.59, D(G(z)): 0.38\n",
      "Epoch [157/5], Step [300 / 600], d_loss: 1.1345, g_loss: 1.1448, D(x): 0.63, D(G(z)): 0.43\n",
      "Epoch [157/5], Step [600 / 600], d_loss: 1.1749, g_loss: 1.0420, D(x): 0.63, D(G(z)): 0.45\n",
      "Epoch [158/5], Step [300 / 600], d_loss: 1.2755, g_loss: 1.1721, D(x): 0.62, D(G(z)): 0.47\n",
      "Epoch [158/5], Step [600 / 600], d_loss: 1.2654, g_loss: 1.0280, D(x): 0.51, D(G(z)): 0.36\n",
      "Epoch [159/5], Step [300 / 600], d_loss: 1.1623, g_loss: 1.2631, D(x): 0.58, D(G(z)): 0.36\n",
      "Epoch [159/5], Step [600 / 600], d_loss: 1.3366, g_loss: 1.0225, D(x): 0.58, D(G(z)): 0.46\n",
      "Epoch [160/5], Step [300 / 600], d_loss: 1.1741, g_loss: 1.1062, D(x): 0.62, D(G(z)): 0.43\n",
      "Epoch [160/5], Step [600 / 600], d_loss: 1.3383, g_loss: 1.2598, D(x): 0.55, D(G(z)): 0.42\n",
      "Epoch [161/5], Step [300 / 600], d_loss: 1.1629, g_loss: 1.0022, D(x): 0.64, D(G(z)): 0.43\n",
      "Epoch [161/5], Step [600 / 600], d_loss: 0.9613, g_loss: 1.0796, D(x): 0.69, D(G(z)): 0.37\n",
      "Epoch [162/5], Step [300 / 600], d_loss: 1.2056, g_loss: 1.3597, D(x): 0.62, D(G(z)): 0.42\n",
      "Epoch [162/5], Step [600 / 600], d_loss: 1.0593, g_loss: 1.2720, D(x): 0.62, D(G(z)): 0.36\n",
      "Epoch [163/5], Step [300 / 600], d_loss: 1.1377, g_loss: 0.9648, D(x): 0.61, D(G(z)): 0.41\n",
      "Epoch [163/5], Step [600 / 600], d_loss: 1.1462, g_loss: 1.0273, D(x): 0.60, D(G(z)): 0.39\n",
      "Epoch [164/5], Step [300 / 600], d_loss: 1.2273, g_loss: 0.9833, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [164/5], Step [600 / 600], d_loss: 1.2144, g_loss: 0.9594, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [165/5], Step [300 / 600], d_loss: 1.1471, g_loss: 1.0785, D(x): 0.66, D(G(z)): 0.43\n",
      "Epoch [165/5], Step [600 / 600], d_loss: 1.2221, g_loss: 1.0370, D(x): 0.64, D(G(z)): 0.46\n",
      "Epoch [166/5], Step [300 / 600], d_loss: 1.0459, g_loss: 0.9896, D(x): 0.68, D(G(z)): 0.39\n",
      "Epoch [166/5], Step [600 / 600], d_loss: 1.1568, g_loss: 0.9645, D(x): 0.62, D(G(z)): 0.40\n",
      "Epoch [167/5], Step [300 / 600], d_loss: 1.0222, g_loss: 1.0390, D(x): 0.61, D(G(z)): 0.33\n",
      "Epoch [167/5], Step [600 / 600], d_loss: 1.0705, g_loss: 1.1460, D(x): 0.65, D(G(z)): 0.38\n",
      "Epoch [168/5], Step [300 / 600], d_loss: 1.1350, g_loss: 1.1744, D(x): 0.58, D(G(z)): 0.38\n",
      "Epoch [168/5], Step [600 / 600], d_loss: 1.1825, g_loss: 1.1825, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [169/5], Step [300 / 600], d_loss: 1.1492, g_loss: 1.2522, D(x): 0.63, D(G(z)): 0.40\n",
      "Epoch [169/5], Step [600 / 600], d_loss: 1.1089, g_loss: 1.2694, D(x): 0.58, D(G(z)): 0.34\n",
      "Epoch [170/5], Step [300 / 600], d_loss: 1.0366, g_loss: 1.1500, D(x): 0.64, D(G(z)): 0.38\n",
      "Epoch [170/5], Step [600 / 600], d_loss: 1.2544, g_loss: 0.9335, D(x): 0.65, D(G(z)): 0.48\n",
      "Epoch [171/5], Step [300 / 600], d_loss: 1.2655, g_loss: 0.8565, D(x): 0.58, D(G(z)): 0.43\n",
      "Epoch [171/5], Step [600 / 600], d_loss: 1.0619, g_loss: 1.0593, D(x): 0.65, D(G(z)): 0.40\n",
      "Epoch [172/5], Step [300 / 600], d_loss: 1.1292, g_loss: 1.2799, D(x): 0.63, D(G(z)): 0.40\n",
      "Epoch [172/5], Step [600 / 600], d_loss: 1.1262, g_loss: 1.2639, D(x): 0.59, D(G(z)): 0.36\n",
      "Epoch [173/5], Step [300 / 600], d_loss: 1.1645, g_loss: 1.0249, D(x): 0.65, D(G(z)): 0.45\n",
      "Epoch [173/5], Step [600 / 600], d_loss: 1.0782, g_loss: 1.1558, D(x): 0.67, D(G(z)): 0.42\n",
      "Epoch [174/5], Step [300 / 600], d_loss: 1.0957, g_loss: 1.2022, D(x): 0.64, D(G(z)): 0.40\n",
      "Epoch [174/5], Step [600 / 600], d_loss: 1.1757, g_loss: 1.0911, D(x): 0.63, D(G(z)): 0.43\n",
      "Epoch [175/5], Step [300 / 600], d_loss: 1.2272, g_loss: 1.1750, D(x): 0.55, D(G(z)): 0.36\n",
      "Epoch [175/5], Step [600 / 600], d_loss: 1.1321, g_loss: 1.2479, D(x): 0.57, D(G(z)): 0.36\n",
      "Epoch [176/5], Step [300 / 600], d_loss: 1.0665, g_loss: 1.0778, D(x): 0.65, D(G(z)): 0.40\n",
      "Epoch [176/5], Step [600 / 600], d_loss: 1.3330, g_loss: 1.1108, D(x): 0.54, D(G(z)): 0.40\n",
      "Epoch [177/5], Step [300 / 600], d_loss: 1.2402, g_loss: 1.0112, D(x): 0.58, D(G(z)): 0.41\n",
      "Epoch [177/5], Step [600 / 600], d_loss: 1.2767, g_loss: 0.9829, D(x): 0.58, D(G(z)): 0.43\n",
      "Epoch [178/5], Step [300 / 600], d_loss: 1.1799, g_loss: 0.9670, D(x): 0.59, D(G(z)): 0.41\n",
      "Epoch [178/5], Step [600 / 600], d_loss: 1.2472, g_loss: 1.0229, D(x): 0.59, D(G(z)): 0.44\n",
      "Epoch [179/5], Step [300 / 600], d_loss: 1.2349, g_loss: 1.0571, D(x): 0.55, D(G(z)): 0.38\n",
      "Epoch [179/5], Step [600 / 600], d_loss: 1.1705, g_loss: 1.0878, D(x): 0.54, D(G(z)): 0.34\n",
      "Epoch [180/5], Step [300 / 600], d_loss: 1.1048, g_loss: 1.1129, D(x): 0.58, D(G(z)): 0.35\n",
      "Epoch [180/5], Step [600 / 600], d_loss: 1.0844, g_loss: 0.9557, D(x): 0.65, D(G(z)): 0.41\n",
      "Epoch [181/5], Step [300 / 600], d_loss: 1.2870, g_loss: 1.0491, D(x): 0.57, D(G(z)): 0.43\n",
      "Epoch [181/5], Step [600 / 600], d_loss: 1.1463, g_loss: 1.1573, D(x): 0.62, D(G(z)): 0.41\n",
      "Epoch [182/5], Step [300 / 600], d_loss: 1.0496, g_loss: 1.1211, D(x): 0.61, D(G(z)): 0.35\n",
      "Epoch [182/5], Step [600 / 600], d_loss: 1.3056, g_loss: 1.1534, D(x): 0.56, D(G(z)): 0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/5], Step [300 / 600], d_loss: 1.1504, g_loss: 1.1024, D(x): 0.63, D(G(z)): 0.43\n",
      "Epoch [183/5], Step [600 / 600], d_loss: 1.1382, g_loss: 1.1329, D(x): 0.59, D(G(z)): 0.39\n",
      "Epoch [184/5], Step [300 / 600], d_loss: 1.1150, g_loss: 1.1487, D(x): 0.60, D(G(z)): 0.37\n",
      "Epoch [184/5], Step [600 / 600], d_loss: 1.1593, g_loss: 0.9694, D(x): 0.62, D(G(z)): 0.41\n",
      "Epoch [185/5], Step [300 / 600], d_loss: 1.2260, g_loss: 1.2829, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [185/5], Step [600 / 600], d_loss: 1.2560, g_loss: 1.1566, D(x): 0.55, D(G(z)): 0.39\n",
      "Epoch [186/5], Step [300 / 600], d_loss: 1.1718, g_loss: 0.9140, D(x): 0.63, D(G(z)): 0.44\n",
      "Epoch [186/5], Step [600 / 600], d_loss: 1.0827, g_loss: 1.0664, D(x): 0.62, D(G(z)): 0.37\n",
      "Epoch [187/5], Step [300 / 600], d_loss: 1.1391, g_loss: 0.9520, D(x): 0.61, D(G(z)): 0.40\n",
      "Epoch [187/5], Step [600 / 600], d_loss: 1.2663, g_loss: 1.1947, D(x): 0.59, D(G(z)): 0.43\n",
      "Epoch [188/5], Step [300 / 600], d_loss: 1.2351, g_loss: 1.1382, D(x): 0.57, D(G(z)): 0.42\n",
      "Epoch [188/5], Step [600 / 600], d_loss: 1.1324, g_loss: 1.0406, D(x): 0.62, D(G(z)): 0.40\n",
      "Epoch [189/5], Step [300 / 600], d_loss: 1.1145, g_loss: 1.2655, D(x): 0.62, D(G(z)): 0.40\n",
      "Epoch [189/5], Step [600 / 600], d_loss: 1.1071, g_loss: 1.2708, D(x): 0.60, D(G(z)): 0.36\n",
      "Epoch [190/5], Step [300 / 600], d_loss: 1.3072, g_loss: 0.9714, D(x): 0.58, D(G(z)): 0.45\n",
      "Epoch [190/5], Step [600 / 600], d_loss: 1.2298, g_loss: 1.0409, D(x): 0.60, D(G(z)): 0.40\n",
      "Epoch [191/5], Step [300 / 600], d_loss: 1.0673, g_loss: 1.1750, D(x): 0.60, D(G(z)): 0.35\n",
      "Epoch [191/5], Step [600 / 600], d_loss: 1.0906, g_loss: 1.1153, D(x): 0.60, D(G(z)): 0.37\n",
      "Epoch [192/5], Step [300 / 600], d_loss: 1.0984, g_loss: 1.0330, D(x): 0.62, D(G(z)): 0.40\n",
      "Epoch [192/5], Step [600 / 600], d_loss: 1.1022, g_loss: 0.9801, D(x): 0.64, D(G(z)): 0.41\n",
      "Epoch [193/5], Step [300 / 600], d_loss: 1.2398, g_loss: 1.0580, D(x): 0.56, D(G(z)): 0.39\n",
      "Epoch [193/5], Step [600 / 600], d_loss: 1.1788, g_loss: 1.0254, D(x): 0.62, D(G(z)): 0.43\n",
      "Epoch [194/5], Step [300 / 600], d_loss: 1.0821, g_loss: 1.2206, D(x): 0.57, D(G(z)): 0.33\n",
      "Epoch [194/5], Step [600 / 600], d_loss: 1.2221, g_loss: 1.1135, D(x): 0.56, D(G(z)): 0.40\n",
      "Epoch [195/5], Step [300 / 600], d_loss: 1.1756, g_loss: 1.0609, D(x): 0.58, D(G(z)): 0.38\n",
      "Epoch [195/5], Step [600 / 600], d_loss: 0.9963, g_loss: 1.2532, D(x): 0.61, D(G(z)): 0.32\n",
      "Epoch [196/5], Step [300 / 600], d_loss: 1.1882, g_loss: 1.0553, D(x): 0.59, D(G(z)): 0.40\n",
      "Epoch [196/5], Step [600 / 600], d_loss: 1.1258, g_loss: 1.0002, D(x): 0.61, D(G(z)): 0.41\n",
      "Epoch [197/5], Step [300 / 600], d_loss: 1.1586, g_loss: 1.0353, D(x): 0.64, D(G(z)): 0.43\n",
      "Epoch [197/5], Step [600 / 600], d_loss: 1.0761, g_loss: 0.9806, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [198/5], Step [300 / 600], d_loss: 1.2586, g_loss: 0.9632, D(x): 0.58, D(G(z)): 0.44\n",
      "Epoch [198/5], Step [600 / 600], d_loss: 1.0937, g_loss: 1.0224, D(x): 0.64, D(G(z)): 0.41\n",
      "Epoch [199/5], Step [300 / 600], d_loss: 1.2207, g_loss: 1.1219, D(x): 0.57, D(G(z)): 0.39\n",
      "Epoch [199/5], Step [600 / 600], d_loss: 1.2797, g_loss: 1.0344, D(x): 0.65, D(G(z)): 0.49\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for epoch in range(200):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        #Build mini-batch dataset\n",
    "        images = images.view(images.size(0), -1)\n",
    "        images = Variable(images)\n",
    "        real_labels = Variable(torch.ones(images.size(0)))\n",
    "        fake_labels = Variable(torch.zeros(images.size(0)))\n",
    "        \n",
    "        #train the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        if has_gpu:\n",
    "            outputs = discriminator(images.cuda())\n",
    "            real_loss = criterion(outputs, real_labels.cuda())\n",
    "            real_score = outputs\n",
    "        else:\n",
    "            outputs = discriminator(images)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_score = outputs\n",
    "        \n",
    "        noise = Variable(torch.randn(images.size(0), 128))\n",
    "        if has_gpu:\n",
    "            fake_images = generator(noise.cuda())\n",
    "            outputs = discriminator(fake_images.detach())\n",
    "            fake_loss = criterion(outputs, fake_labels.cuda())\n",
    "            fake_score = outputs\n",
    "        else:\n",
    "            fake_images = generator(noise)\n",
    "            outputs = discriminator(fake_images.detach())\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_score = outputs\n",
    "        \n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #train the generator\n",
    "        generator.zero_grad()\n",
    "        noise = Variable(torch.randn(images.size(0), 128))\n",
    "        if has_gpu:\n",
    "            fake_images = generator(noise.cuda())\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels.cuda())\n",
    "        else:\n",
    "            fake_images = generator(noise)\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 300 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{} / {}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}\".format(\n",
    "                epoch, 200, i+1, 600, d_loss.data[0], g_loss.data[0], real_score.data.mean(), fake_score.cuda().data.mean()))\n",
    "            \n",
    "    #save the sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    torchvision.utils.save_image(fake_images.data, './gen/fake_samples_{}.png'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Now we save the models to resume training later / use to generate similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), './gen.pkl')\n",
    "torch.save(discriminator.state_dict(), './dis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A peek at the Generator's output\n",
    "\n",
    "Here take a look at one of the outputs from the Generator.\n",
    "\n",
    "![Generated digits](./images/fake_samples_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although the generated digits look **awesome**, they can be further improved by:\n",
    "\n",
    "1. Tweaking the hyperparameters like batch size, adding dropout & batchnorm layers etc.\n",
    "2. Using Convolutional layers instead of fully-connected layers (DCGANs)\n",
    "2. Training further. Most of the awesome results that I have seen so far are from extensive training for days if not weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
